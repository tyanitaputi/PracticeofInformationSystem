{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_TextMining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVxs85he106s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "83179cb2-2e3e-4984-d0ee-7ee61e3b8157"
      },
      "source": [
        "\n",
        "# install necessary dependencies\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig25mmP630Dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3886a401-e066-46dd-cca8-358b0b147382"
      },
      "source": [
        "#Pre-processing\n",
        "\n",
        "documents = [\"The implementation of FEMA Whole Community initiative is an opportunity to engage the whole of societal capital in disaster management.\",\n",
        "             \"Its policies are gradually being developed, questions are mounting about what implications this new strategy will have for the traditional intergovernmental management of disasters.\",\n",
        "             \"As an emerging issue in disaster science and management, the following chapter examines what governance issues are likely to arise from the adoption of the Whole Community approach.\",\n",
        "             \"An exchange between an academic and a practitioner provided unique perspectives, and a collaborative effort identified where there was agreement and where perspectives diverged.\",\n",
        "             \"The academic segment reviewed the literature on key theoretical concepts, which define government legitimacy to engage in emergency management in the first place, identify boundaries of its mandate, and explain indicators that must be considered when evaluating a governance system.\",\n",
        "             \"The academic segment concluded that increasing delegation of responsibilities for disaster management to partners in a widespread network poses significant challenges for government.\",\n",
        "             \"Moving toward a Whole Community of shared responsibilities by extending networks even further and maintaining network relationships for goods and services that are largely not contract-based and thus not enforceable, government will be increasingly challenged to fulfill its commitment of good governance in terms of displaying maximum predictability, transparency, and accountability.\",\n",
        "             \"Clarifications are needed on concrete Whole Community policies to analyze these challenges further and for research to make contributions to the ongoing policy discussions.\",\n",
        "             \"The practitioner segment begins by discussing how the language of this new approach has been woven into federal emergency management doctrine.\",\n",
        "             \"The ability of this language to actually affect shared responsibility is analyzed based on past similar programs, given that this segment concludes that the Whole Community approach has delivered nothing actionable that achieves devolution.\"]\n",
        "\n",
        "#Cleaning\n",
        "import re\n",
        "\n",
        "def cleaning_text(text):\n",
        "    # delete '\n",
        "    pattern2 = ''\n",
        "    text = re.sub(pattern2, '', text)    \n",
        "    #delete ,\n",
        "    pattern3 = ',' \n",
        "    text = re.sub(pattern3, '', text)\n",
        "    return text\n",
        "  \n",
        "\n",
        "for text in documents:\n",
        "    print(cleaning_text(text))\n",
        "\n",
        "#Tokenize\n",
        "def tokenize_text(text):\n",
        "  text = re.sub('[.,]', '', text)\n",
        "  return text.split()\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  print(tokenize_text(text))\n",
        "\n",
        "#Stemming and Lemmatize\n",
        "from nltk.corpus import wordnet as wn # import lemmatize\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    # make words lower  e.g., Python =>python\n",
        "    word=word.lower()\n",
        "    \n",
        "    # lemmatize  e.g., cooked=>cook\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "      return lemma\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  print([lemmatize_word(word) for word in tokens])\n",
        "\n",
        "#remove stop words\n",
        "\n",
        "en_stop = nltk.corpus.stopwords.words('english')\n",
        "print(en_stop)\n",
        "\n",
        "\n",
        "def remove_stopwords(word, stopwordset):\n",
        "  if word in stopwordset:\n",
        "    return None\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "for text in documents:\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  print([remove_stopwords(word, en_stop) for word in tokens])\n",
        "\n",
        "def preprocessing_text(text):\n",
        "  text = cleaning_text(text)\n",
        "  tokens = tokenize_text(text)\n",
        "  tokens = [lemmatize_word(word) for word in tokens]\n",
        "  tokens = [remove_stopwords(word, en_stop) for word in tokens]\n",
        "  tokens = [word for word in tokens if word is not None]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "preprocessed_docs = [preprocessing_text(text) for text in documents]\n",
        "preprocessed_docs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The implementation of FEMA Whole Community initiative is an opportunity to engage the whole of societal capital in disaster management.\n",
            "Its policies are gradually being developed questions are mounting about what implications this new strategy will have for the traditional intergovernmental management of disasters.\n",
            "As an emerging issue in disaster science and management the following chapter examines what governance issues are likely to arise from the adoption of the Whole Community approach.\n",
            "An exchange between an academic and a practitioner provided unique perspectives and a collaborative effort identified where there was agreement and where perspectives diverged.\n",
            "The academic segment reviewed the literature on key theoretical concepts which define government legitimacy to engage in emergency management in the first place identify boundaries of its mandate and explain indicators that must be considered when evaluating a governance system.\n",
            "The academic segment concluded that increasing delegation of responsibilities for disaster management to partners in a widespread network poses significant challenges for government.\n",
            "Moving toward a Whole Community of shared responsibilities by extending networks even further and maintaining network relationships for goods and services that are largely not contract-based and thus not enforceable government will be increasingly challenged to fulfill its commitment of good governance in terms of displaying maximum predictability transparency and accountability.\n",
            "Clarifications are needed on concrete Whole Community policies to analyze these challenges further and for research to make contributions to the ongoing policy discussions.\n",
            "The practitioner segment begins by discussing how the language of this new approach has been woven into federal emergency management doctrine.\n",
            "The ability of this language to actually affect shared responsibility is analyzed based on past similar programs given that this segment concludes that the Whole Community approach has delivered nothing actionable that achieves devolution.\n",
            "['The', 'implementation', 'of', 'FEMA', 'Whole', 'Community', 'initiative', 'is', 'an', 'opportunity', 'to', 'engage', 'the', 'whole', 'of', 'societal', 'capital', 'in', 'disaster', 'management']\n",
            "['Its', 'policies', 'are', 'gradually', 'being', 'developed', 'questions', 'are', 'mounting', 'about', 'what', 'implications', 'this', 'new', 'strategy', 'will', 'have', 'for', 'the', 'traditional', 'intergovernmental', 'management', 'of', 'disasters']\n",
            "['As', 'an', 'emerging', 'issue', 'in', 'disaster', 'science', 'and', 'management', 'the', 'following', 'chapter', 'examines', 'what', 'governance', 'issues', 'are', 'likely', 'to', 'arise', 'from', 'the', 'adoption', 'of', 'the', 'Whole', 'Community', 'approach']\n",
            "['An', 'exchange', 'between', 'an', 'academic', 'and', 'a', 'practitioner', 'provided', 'unique', 'perspectives', 'and', 'a', 'collaborative', 'effort', 'identified', 'where', 'there', 'was', 'agreement', 'and', 'where', 'perspectives', 'diverged']\n",
            "['The', 'academic', 'segment', 'reviewed', 'the', 'literature', 'on', 'key', 'theoretical', 'concepts', 'which', 'define', 'government', 'legitimacy', 'to', 'engage', 'in', 'emergency', 'management', 'in', 'the', 'first', 'place', 'identify', 'boundaries', 'of', 'its', 'mandate', 'and', 'explain', 'indicators', 'that', 'must', 'be', 'considered', 'when', 'evaluating', 'a', 'governance', 'system']\n",
            "['The', 'academic', 'segment', 'concluded', 'that', 'increasing', 'delegation', 'of', 'responsibilities', 'for', 'disaster', 'management', 'to', 'partners', 'in', 'a', 'widespread', 'network', 'poses', 'significant', 'challenges', 'for', 'government']\n",
            "['Moving', 'toward', 'a', 'Whole', 'Community', 'of', 'shared', 'responsibilities', 'by', 'extending', 'networks', 'even', 'further', 'and', 'maintaining', 'network', 'relationships', 'for', 'goods', 'and', 'services', 'that', 'are', 'largely', 'not', 'contract-based', 'and', 'thus', 'not', 'enforceable', 'government', 'will', 'be', 'increasingly', 'challenged', 'to', 'fulfill', 'its', 'commitment', 'of', 'good', 'governance', 'in', 'terms', 'of', 'displaying', 'maximum', 'predictability', 'transparency', 'and', 'accountability']\n",
            "['Clarifications', 'are', 'needed', 'on', 'concrete', 'Whole', 'Community', 'policies', 'to', 'analyze', 'these', 'challenges', 'further', 'and', 'for', 'research', 'to', 'make', 'contributions', 'to', 'the', 'ongoing', 'policy', 'discussions']\n",
            "['The', 'practitioner', 'segment', 'begins', 'by', 'discussing', 'how', 'the', 'language', 'of', 'this', 'new', 'approach', 'has', 'been', 'woven', 'into', 'federal', 'emergency', 'management', 'doctrine']\n",
            "['The', 'ability', 'of', 'this', 'language', 'to', 'actually', 'affect', 'shared', 'responsibility', 'is', 'analyzed', 'based', 'on', 'past', 'similar', 'programs', 'given', 'that', 'this', 'segment', 'concludes', 'that', 'the', 'Whole', 'Community', 'approach', 'has', 'delivered', 'nothing', 'actionable', 'that', 'achieves', 'devolution']\n",
            "['the', 'implementation', 'of', 'fema', 'whole', 'community', 'initiative', 'be', 'an', 'opportunity', 'to', 'engage', 'the', 'whole', 'of', 'societal', 'capital', 'in', 'disaster', 'management']\n",
            "['it', 'policy', 'are', 'gradually', 'being', 'develop', 'question', 'are', 'mounting', 'about', 'what', 'implication', 'this', 'new', 'strategy', 'will', 'have', 'for', 'the', 'traditional', 'intergovernmental', 'management', 'of', 'disaster']\n",
            "['as', 'an', 'emerge', 'issue', 'in', 'disaster', 'science', 'and', 'management', 'the', 'following', 'chapter', 'examine', 'what', 'governance', 'issue', 'are', 'likely', 'to', 'arise', 'from', 'the', 'adoption', 'of', 'the', 'whole', 'community', 'approach']\n",
            "['an', 'exchange', 'between', 'an', 'academic', 'and', 'a', 'practitioner', 'provide', 'unique', 'perspective', 'and', 'a', 'collaborative', 'effort', 'identify', 'where', 'there', 'wa', 'agreement', 'and', 'where', 'perspective', 'diverge']\n",
            "['the', 'academic', 'segment', 'review', 'the', 'literature', 'on', 'key', 'theoretical', 'concept', 'which', 'define', 'government', 'legitimacy', 'to', 'engage', 'in', 'emergency', 'management', 'in', 'the', 'first', 'place', 'identify', 'boundary', 'of', 'it', 'mandate', 'and', 'explain', 'indicator', 'that', 'must', 'be', 'consider', 'when', 'evaluate', 'a', 'governance', 'system']\n",
            "['the', 'academic', 'segment', 'conclude', 'that', 'increase', 'delegation', 'of', 'responsibility', 'for', 'disaster', 'management', 'to', 'partner', 'in', 'a', 'widespread', 'network', 'pose', 'significant', 'challenge', 'for', 'government']\n",
            "['move', 'toward', 'a', 'whole', 'community', 'of', 'share', 'responsibility', 'by', 'extend', 'network', 'even', 'further', 'and', 'maintain', 'network', 'relationship', 'for', 'good', 'and', 'services', 'that', 'are', 'largely', 'not', 'contract-based', 'and', 'thus', 'not', 'enforceable', 'government', 'will', 'be', 'increasingly', 'challenge', 'to', 'fulfill', 'it', 'commitment', 'of', 'good', 'governance', 'in', 'terms', 'of', 'display', 'maximum', 'predictability', 'transparency', 'and', 'accountability']\n",
            "['clarification', 'are', 'need', 'on', 'concrete', 'whole', 'community', 'policy', 'to', 'analyze', 'these', 'challenge', 'further', 'and', 'for', 'research', 'to', 'make', 'contribution', 'to', 'the', 'ongoing', 'policy', 'discussion']\n",
            "['the', 'practitioner', 'segment', 'begin', 'by', 'discuss', 'how', 'the', 'language', 'of', 'this', 'new', 'approach', 'ha', 'be', 'weave', 'into', 'federal', 'emergency', 'management', 'doctrine']\n",
            "['the', 'ability', 'of', 'this', 'language', 'to', 'actually', 'affect', 'share', 'responsibility', 'be', 'analyze', 'base', 'on', 'past', 'similar', 'program', 'given', 'that', 'this', 'segment', 'conclude', 'that', 'the', 'whole', 'community', 'approach', 'ha', 'deliver', 'nothing', 'actionable', 'that', 'achieve', 'devolution']\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "[None, 'implementation', None, 'fema', 'whole', 'community', 'initiative', None, None, 'opportunity', None, 'engage', None, 'whole', None, 'societal', 'capital', None, 'disaster', 'management']\n",
            "[None, 'policy', None, 'gradually', None, 'develop', 'question', None, 'mounting', None, None, 'implication', None, 'new', 'strategy', None, None, None, None, 'traditional', 'intergovernmental', 'management', None, 'disaster']\n",
            "[None, None, 'emerge', 'issue', None, 'disaster', 'science', None, 'management', None, 'following', 'chapter', 'examine', None, 'governance', 'issue', None, 'likely', None, 'arise', None, None, 'adoption', None, None, 'whole', 'community', 'approach']\n",
            "[None, 'exchange', None, None, 'academic', None, None, 'practitioner', 'provide', 'unique', 'perspective', None, None, 'collaborative', 'effort', 'identify', None, None, 'wa', 'agreement', None, None, 'perspective', 'diverge']\n",
            "[None, 'academic', 'segment', 'review', None, 'literature', None, 'key', 'theoretical', 'concept', None, 'define', 'government', 'legitimacy', None, 'engage', None, 'emergency', 'management', None, None, 'first', 'place', 'identify', 'boundary', None, None, 'mandate', None, 'explain', 'indicator', None, 'must', None, 'consider', None, 'evaluate', None, 'governance', 'system']\n",
            "[None, 'academic', 'segment', 'conclude', None, 'increase', 'delegation', None, 'responsibility', None, 'disaster', 'management', None, 'partner', None, None, 'widespread', 'network', 'pose', 'significant', 'challenge', None, 'government']\n",
            "['move', 'toward', None, 'whole', 'community', None, 'share', 'responsibility', None, 'extend', 'network', 'even', None, None, 'maintain', 'network', 'relationship', None, 'good', None, 'services', None, None, 'largely', None, 'contract-based', None, 'thus', None, 'enforceable', 'government', None, None, 'increasingly', 'challenge', None, 'fulfill', None, 'commitment', None, 'good', 'governance', None, 'terms', None, 'display', 'maximum', 'predictability', 'transparency', None, 'accountability']\n",
            "['clarification', None, 'need', None, 'concrete', 'whole', 'community', 'policy', None, 'analyze', None, 'challenge', None, None, None, 'research', None, 'make', 'contribution', None, None, 'ongoing', 'policy', 'discussion']\n",
            "[None, 'practitioner', 'segment', 'begin', None, 'discuss', None, None, 'language', None, None, 'new', 'approach', 'ha', None, 'weave', None, 'federal', 'emergency', 'management', 'doctrine']\n",
            "[None, 'ability', None, None, 'language', None, 'actually', 'affect', 'share', 'responsibility', None, 'analyze', 'base', None, 'past', 'similar', 'program', 'given', None, None, 'segment', 'conclude', None, None, 'whole', 'community', 'approach', 'ha', 'deliver', 'nothing', 'actionable', None, 'achieve', 'devolution']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['implementation',\n",
              "  'fema',\n",
              "  'whole',\n",
              "  'community',\n",
              "  'initiative',\n",
              "  'opportunity',\n",
              "  'engage',\n",
              "  'whole',\n",
              "  'societal',\n",
              "  'capital',\n",
              "  'disaster',\n",
              "  'management'],\n",
              " ['policy',\n",
              "  'gradually',\n",
              "  'develop',\n",
              "  'question',\n",
              "  'mounting',\n",
              "  'implication',\n",
              "  'new',\n",
              "  'strategy',\n",
              "  'traditional',\n",
              "  'intergovernmental',\n",
              "  'management',\n",
              "  'disaster'],\n",
              " ['emerge',\n",
              "  'issue',\n",
              "  'disaster',\n",
              "  'science',\n",
              "  'management',\n",
              "  'following',\n",
              "  'chapter',\n",
              "  'examine',\n",
              "  'governance',\n",
              "  'issue',\n",
              "  'likely',\n",
              "  'arise',\n",
              "  'adoption',\n",
              "  'whole',\n",
              "  'community',\n",
              "  'approach'],\n",
              " ['exchange',\n",
              "  'academic',\n",
              "  'practitioner',\n",
              "  'provide',\n",
              "  'unique',\n",
              "  'perspective',\n",
              "  'collaborative',\n",
              "  'effort',\n",
              "  'identify',\n",
              "  'wa',\n",
              "  'agreement',\n",
              "  'perspective',\n",
              "  'diverge'],\n",
              " ['academic',\n",
              "  'segment',\n",
              "  'review',\n",
              "  'literature',\n",
              "  'key',\n",
              "  'theoretical',\n",
              "  'concept',\n",
              "  'define',\n",
              "  'government',\n",
              "  'legitimacy',\n",
              "  'engage',\n",
              "  'emergency',\n",
              "  'management',\n",
              "  'first',\n",
              "  'place',\n",
              "  'identify',\n",
              "  'boundary',\n",
              "  'mandate',\n",
              "  'explain',\n",
              "  'indicator',\n",
              "  'must',\n",
              "  'consider',\n",
              "  'evaluate',\n",
              "  'governance',\n",
              "  'system'],\n",
              " ['academic',\n",
              "  'segment',\n",
              "  'conclude',\n",
              "  'increase',\n",
              "  'delegation',\n",
              "  'responsibility',\n",
              "  'disaster',\n",
              "  'management',\n",
              "  'partner',\n",
              "  'widespread',\n",
              "  'network',\n",
              "  'pose',\n",
              "  'significant',\n",
              "  'challenge',\n",
              "  'government'],\n",
              " ['move',\n",
              "  'toward',\n",
              "  'whole',\n",
              "  'community',\n",
              "  'share',\n",
              "  'responsibility',\n",
              "  'extend',\n",
              "  'network',\n",
              "  'even',\n",
              "  'maintain',\n",
              "  'network',\n",
              "  'relationship',\n",
              "  'good',\n",
              "  'services',\n",
              "  'largely',\n",
              "  'contract-based',\n",
              "  'thus',\n",
              "  'enforceable',\n",
              "  'government',\n",
              "  'increasingly',\n",
              "  'challenge',\n",
              "  'fulfill',\n",
              "  'commitment',\n",
              "  'good',\n",
              "  'governance',\n",
              "  'terms',\n",
              "  'display',\n",
              "  'maximum',\n",
              "  'predictability',\n",
              "  'transparency',\n",
              "  'accountability'],\n",
              " ['clarification',\n",
              "  'need',\n",
              "  'concrete',\n",
              "  'whole',\n",
              "  'community',\n",
              "  'policy',\n",
              "  'analyze',\n",
              "  'challenge',\n",
              "  'research',\n",
              "  'make',\n",
              "  'contribution',\n",
              "  'ongoing',\n",
              "  'policy',\n",
              "  'discussion'],\n",
              " ['practitioner',\n",
              "  'segment',\n",
              "  'begin',\n",
              "  'discuss',\n",
              "  'language',\n",
              "  'new',\n",
              "  'approach',\n",
              "  'ha',\n",
              "  'weave',\n",
              "  'federal',\n",
              "  'emergency',\n",
              "  'management',\n",
              "  'doctrine'],\n",
              " ['ability',\n",
              "  'language',\n",
              "  'actually',\n",
              "  'affect',\n",
              "  'share',\n",
              "  'responsibility',\n",
              "  'analyze',\n",
              "  'base',\n",
              "  'past',\n",
              "  'similar',\n",
              "  'program',\n",
              "  'given',\n",
              "  'segment',\n",
              "  'conclude',\n",
              "  'whole',\n",
              "  'community',\n",
              "  'approach',\n",
              "  'ha',\n",
              "  'deliver',\n",
              "  'nothing',\n",
              "  'actionable',\n",
              "  'achieve',\n",
              "  'devolution']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA-rYKT3y3ht",
        "colab_type": "text"
      },
      "source": [
        "Preporcessing the documents using Clearning, Tokenize, Stemming, and Remove stop words. We got 124 words for all documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKY6RxvH2OY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use the preprocessed documents\n",
        "doc1 = set(['implementation','fema','whole','community','initiative','opportunity','engage','whole','societal','capital','disaster','management'])\n",
        "doc2 = set(['policy','gradually','develop','question','mounting','implication','new','strategy','traditional','intergovernmental','management','disaster'])\n",
        "doc3 = set(['emerge','issue','disaster','science','management','following','chapter','examine','governance','issue','likely','arise','adoption','whole','community','approach'])\n",
        "doc4 = set(['exchange','academic','practitioner','provide','unique','perspective','collaborative','effort','identify','wa','agreement','perspective','diverge'])\n",
        "doc5 = set(['academic','segment','review','literature','key','theoretical','concept','define','government','legitimacy','engage','emergency','management','first','place','identify','boundary','mandate','explain','indicator','must','consider','evaluate','governance','system'])\n",
        "doc6 = set(['academic','segment','conclude','increase','delegation','responsibility','disaster','management','partner','widespread','network','pose','significant','challenge','government'])\n",
        "doc7 = set(['move','toward','whole','community','share','responsibility','extend','network','even','maintain','network','relationship','good','services','largely','contract-based','thus','enforceable','government','increasingly','challenge','fulfill','commitment','good','governance','terms','display','maximum','predictability','transparency','accountability'])\n",
        "doc8 = set(['clarification','need','concrete','whole','community','policy','analyze','challenge','research','make','contribution','ongoing','policy','discussion'])\n",
        "doc9 = set(['practitioner','segment','begin','discuss','language','new','approach','ha','weave','federal','emergency','management','doctrine'])\n",
        "doc10 = set(['ability','language','actually','affect','share','responsibility','analyze','base','past','similar','program','given','segment','conclude','whole','community','approach','ha','deliver','nothing','actionable','achieve','devolution'])\n",
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLazSV3W8y2u",
        "colab_type": "text"
      },
      "source": [
        "The first representation method is Bag-of-words. In this method, we represent the appearence of the words in the whole documents. Firstly, we need to convert the text documents in to a vector. Since we have 10 documents, then this method will count how many times each of the word appears. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1d9TIarFz1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d6cea496-919f-4229-bd29-0e7cc13a8f12"
      },
      "source": [
        "#Using Bag-of-words Representation Method\n",
        "def bow_vectorizer(docs):\n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "        \n",
        "  result_list = []\n",
        "  for doc in docs:\n",
        "    doc_vec = [0] * len(word2id)\n",
        "    for w in doc:\n",
        "      doc_vec[word2id[w]] += 1\n",
        "    result_list.append(doc_vec)\n",
        "  return result_list, word2id\n",
        "\n",
        "bow_vec, word2id = bow_vectorizer(preprocessed_docs)\n",
        "print(bow_vec)\n",
        "\n",
        "word2id.items()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('implementation', 0), ('fema', 1), ('whole', 2), ('community', 3), ('initiative', 4), ('opportunity', 5), ('engage', 6), ('societal', 7), ('capital', 8), ('disaster', 9), ('management', 10), ('policy', 11), ('gradually', 12), ('develop', 13), ('question', 14), ('mounting', 15), ('implication', 16), ('new', 17), ('strategy', 18), ('traditional', 19), ('intergovernmental', 20), ('emerge', 21), ('issue', 22), ('science', 23), ('following', 24), ('chapter', 25), ('examine', 26), ('governance', 27), ('likely', 28), ('arise', 29), ('adoption', 30), ('approach', 31), ('exchange', 32), ('academic', 33), ('practitioner', 34), ('provide', 35), ('unique', 36), ('perspective', 37), ('collaborative', 38), ('effort', 39), ('identify', 40), ('wa', 41), ('agreement', 42), ('diverge', 43), ('segment', 44), ('review', 45), ('literature', 46), ('key', 47), ('theoretical', 48), ('concept', 49), ('define', 50), ('government', 51), ('legitimacy', 52), ('emergency', 53), ('first', 54), ('place', 55), ('boundary', 56), ('mandate', 57), ('explain', 58), ('indicator', 59), ('must', 60), ('consider', 61), ('evaluate', 62), ('system', 63), ('conclude', 64), ('increase', 65), ('delegation', 66), ('responsibility', 67), ('partner', 68), ('widespread', 69), ('network', 70), ('pose', 71), ('significant', 72), ('challenge', 73), ('move', 74), ('toward', 75), ('share', 76), ('extend', 77), ('even', 78), ('maintain', 79), ('relationship', 80), ('good', 81), ('services', 82), ('largely', 83), ('contract-based', 84), ('thus', 85), ('enforceable', 86), ('increasingly', 87), ('fulfill', 88), ('commitment', 89), ('terms', 90), ('display', 91), ('maximum', 92), ('predictability', 93), ('transparency', 94), ('accountability', 95), ('clarification', 96), ('need', 97), ('concrete', 98), ('analyze', 99), ('research', 100), ('make', 101), ('contribution', 102), ('ongoing', 103), ('discussion', 104), ('begin', 105), ('discuss', 106), ('language', 107), ('ha', 108), ('weave', 109), ('federal', 110), ('doctrine', 111), ('ability', 112), ('actually', 113), ('affect', 114), ('base', 115), ('past', 116), ('similar', 117), ('program', 118), ('given', 119), ('deliver', 120), ('nothing', 121), ('actionable', 122), ('achieve', 123), ('devolution', 124)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvDNCnLuVPEk",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can see that \n",
        "1. In doc1, the word \"whole\" appears the most which is twice\n",
        "2. In doc2, the words \"disaster\", \"management\", \"policy\", \"gradually\", \"develop\", \"question\", \"mounting\", \"implication\", \"new\", \"strategy\", \"traditional\", \"intergovernmental\" appear the most which are once\n",
        "3. In doc3, the word \"issue\" appears the most which is twice\n",
        "4. In doc4, the word \"perspective\" appears the most which is twice\n",
        "5. In doc5, the words \"engage\", \"management\", \"governance\", \"academic\", \"identify\", \"segment\", \"review\", \"literature\", \"key\", \"theoretical\", \"concept\", \"define\", \"government\", \"legitimacy\", \"emergency\", \"first\", \"place\", \"boundary\", \"mandate\", \"explain\", \"indicator\", \"must\", \"consider\", \"evaluate\", \"system\" appear the most which are once\n",
        "6. In doc6, the words \"disaster\", \"management\", \"academic\", \"segment\", \"government\", \"conclude\", \"increase\", \"delegation\", \"responsibility\", \"partner\", \"widespread\", \"network\", \"pose\", \"significant\", \"challenge\" appear the most which are once\n",
        "7. In doc7, the words \"good\" and \"network\" appear the most which are twice\n",
        "8. In doc8, the word \"policy\" appears the most which is twice\n",
        "9. In doc9, the words \"management\", \"new\", \"approach\", \"practitioner\", \"segment\", \"emergency\", \"discuss\", \"language\", \"weave\", \"federal\", \"doctrine\", \"ability\" appear the most which are once\n",
        "10. In doc10, the words \"whole\", \"community\", \"approach\", \"segment\", \"conclude\", \"responsibility\", \"share\", \"analyze\", \"language\", \"ability\", \"actually\", \"affect\", \"base\", \"past\", \"similar\", \"program\", \"given\", \"deliver\", \"nothing\", \"actionable\", \"achieve\", \"devolution\" appear the most which are once\n",
        "\n",
        "From all documents, we can see that the most appearance of the words are only twice in several documents and the words that appear once spread evenly in some documents. And we can see that the words \"whole\", \"community\", \"disaster\", and \"management\" are the words with the most frequent appearance in several documents. \n",
        "\n",
        "Consideration:\n",
        "the words \"whole\", \"community\", \"disaster\", and \"management\" that the most frequent appearance among all documents indicate the most informative words that being discussed in the whole documents.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1WtHBCC_ZBR",
        "colab_type": "text"
      },
      "source": [
        "The second representation method is TF*IDF Vector. In this method, we calculate degree of importance of each word by scoring them. \n",
        "Firstly we calculate the frequency of the word (t) in the document (TF procedure) and then we do the inverse frequency of the word (t) in all documents and lastly (IDF procedure), we calculate the weight of each word.\n",
        "\n",
        "      TF*IDF(t,d) = TF(t, d) * IDF(t)\n",
        "      TF*IDF(t,d) = TF(t, d) * log (N/DFt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CojVrHUEHoTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "82429c02-56c8-4de9-ff7c-fda7b32afcc3"
      },
      "source": [
        "def tfidf_vectorizer(docs):\n",
        "  def tf(word2id, doc):\n",
        "    term_counts = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      term_counts[word2id[term]] = doc.count(term)\n",
        "    tf_values = list(map(lambda x: x/sum(term_counts), term_counts))\n",
        "    return tf_values\n",
        "  \n",
        "  def idf(word2id, docs):\n",
        "    idf = np.zeros(len(word2id))\n",
        "    for term in word2id.keys():\n",
        "      idf[word2id[term]] = np.log(len(docs) / sum([bool(term in doc) for doc in docs]))\n",
        "    return idf\n",
        "  \n",
        "  word2id = {}\n",
        "  for doc in docs:\n",
        "    for w in doc:\n",
        "      if w not in word2id:\n",
        "        word2id[w] = len(word2id)\n",
        "  \n",
        "  return [[_tf*_idf for _tf, _idf in zip(tf(word2id, doc), idf(word2id, docs))] for doc in docs], word2id\n",
        "\n",
        "tfidf_vector, word2id = tfidf_vectorizer(preprocessed_docs)\n",
        "print(tfidf_vector)\n",
        "print(word2id.items())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.19188209108283716, 0.19188209108283716, 0.11552453009332421, 0.057762265046662105, 0.19188209108283716, 0.19188209108283716, 0.134119826036175, 0.19188209108283716, 0.19188209108283716, 0.07635756098951292, 0.04256880198049923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07635756098951292, 0.04256880198049923, 0.134119826036175, 0.19188209108283716, 0.19188209108283716, 0.19188209108283716, 0.19188209108283716, 0.19188209108283716, 0.134119826036175, 0.19188209108283716, 0.19188209108283716, 0.19188209108283716, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.04332169878499658, 0.04332169878499658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.057268170742134694, 0.03192660148537442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14391156831212787, 0.28782313662425574, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.07524830027037101, 0.14391156831212787, 0.14391156831212787, 0.14391156831212787, 0.07524830027037101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17712193023031123, 0.09261329264045663, 0.12380291634108465, 0.17712193023031123, 0.17712193023031123, 0.35424386046062245, 0.17712193023031123, 0.17712193023031123, 0.12380291634108465, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064377516497364, 0.0, 0.0, 0.0, 0.02043302495063963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.048158912173037444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.048158912173037444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064377516497364, 0.0, 0.0, 0.0, 0.03665162927496621, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.048158912173037444, 0.09210340371976183, 0.064377516497364, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.09210340371976183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06108604879161034, 0.034055041584399384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08026485362172907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06108604879161034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08026485362172907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.10729586082894002, 0.15350567286626973, 0.15350567286626973, 0.08026485362172907, 0.15350567286626973, 0.15350567286626973, 0.10729586082894002, 0.15350567286626973, 0.15350567286626973, 0.08026485362172907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.022359586469675653, 0.022359586469675653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03883783239761084, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03883783239761084, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03883783239761084, 0.0, 0.0, 0.10383470402800647, 0.0, 0.0, 0.03883783239761084, 0.0742769384836789, 0.0742769384836789, 0.051917352014003236, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.1485538769673578, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0742769384836789, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.049510512897138946, 0.049510512897138946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22991970177630003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08599805745185257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.11495985088815001, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.16447036378528898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03929427875123006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12380291634108465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09261329264045663, 0.0, 0.0, 0.12380291634108465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0704839024518581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12380291634108465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17712193023031123, 0.17712193023031123, 0.12380291634108465, 0.12380291634108465, 0.17712193023031123, 0.17712193023031123, 0.17712193023031123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.030136833937388925, 0.030136833937388925, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05234664366634505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039838727472789354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06997556141017827, 0.0, 0.0, 0.05234664366634505, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06997556141017827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06997556141017827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06997556141017827, 0.06997556141017827, 0.0, 0.0, 0.0, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672, 0.1001123953475672]]\n",
            "dict_items([('implementation', 0), ('fema', 1), ('whole', 2), ('community', 3), ('initiative', 4), ('opportunity', 5), ('engage', 6), ('societal', 7), ('capital', 8), ('disaster', 9), ('management', 10), ('policy', 11), ('gradually', 12), ('develop', 13), ('question', 14), ('mounting', 15), ('implication', 16), ('new', 17), ('strategy', 18), ('traditional', 19), ('intergovernmental', 20), ('emerge', 21), ('issue', 22), ('science', 23), ('following', 24), ('chapter', 25), ('examine', 26), ('governance', 27), ('likely', 28), ('arise', 29), ('adoption', 30), ('approach', 31), ('exchange', 32), ('academic', 33), ('practitioner', 34), ('provide', 35), ('unique', 36), ('perspective', 37), ('collaborative', 38), ('effort', 39), ('identify', 40), ('wa', 41), ('agreement', 42), ('diverge', 43), ('segment', 44), ('review', 45), ('literature', 46), ('key', 47), ('theoretical', 48), ('concept', 49), ('define', 50), ('government', 51), ('legitimacy', 52), ('emergency', 53), ('first', 54), ('place', 55), ('boundary', 56), ('mandate', 57), ('explain', 58), ('indicator', 59), ('must', 60), ('consider', 61), ('evaluate', 62), ('system', 63), ('conclude', 64), ('increase', 65), ('delegation', 66), ('responsibility', 67), ('partner', 68), ('widespread', 69), ('network', 70), ('pose', 71), ('significant', 72), ('challenge', 73), ('move', 74), ('toward', 75), ('share', 76), ('extend', 77), ('even', 78), ('maintain', 79), ('relationship', 80), ('good', 81), ('services', 82), ('largely', 83), ('contract-based', 84), ('thus', 85), ('enforceable', 86), ('increasingly', 87), ('fulfill', 88), ('commitment', 89), ('terms', 90), ('display', 91), ('maximum', 92), ('predictability', 93), ('transparency', 94), ('accountability', 95), ('clarification', 96), ('need', 97), ('concrete', 98), ('analyze', 99), ('research', 100), ('make', 101), ('contribution', 102), ('ongoing', 103), ('discussion', 104), ('begin', 105), ('discuss', 106), ('language', 107), ('ha', 108), ('weave', 109), ('federal', 110), ('doctrine', 111), ('ability', 112), ('actually', 113), ('affect', 114), ('base', 115), ('past', 116), ('similar', 117), ('program', 118), ('given', 119), ('deliver', 120), ('nothing', 121), ('actionable', 122), ('achieve', 123), ('devolution', 124)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPIL5Hh3nt-W",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can see that \n",
        "1. In doc1, the word \"implementation\", \"whole\", \"initiative\", \"opportunity\", \"societal\", and \"capital\" have the best score 0.19188209108283716\n",
        "2. In doc2, the words \"gradually\", \"develop\", \"question\", \"mounting\", \"implication\", \"strategy\", \"traditional\", and \"intergovernmental\" have the best score 0.19188209108283716\n",
        "3. In doc3, the word \"issue\" has the best score 0.28782313662425574\n",
        "4. In doc4, the word \"perspective\" has the best score 0.35424386046062245\n",
        "5. In doc5, the words \"review\", \"literature\", \"key\", \"theoretical\", \"concept\", \"define\", \"legitimacy\", \"first\", \"place\", \"boundary\", \"mandate\", \"explain\", \"indicator\", \"must\", \"consider\", \"evaluate\", \"system\" have the best score 0.09210340371976183\n",
        "6. In doc6, the words \"increase\", \"delegation\", \"partner\", \"widespread\",  \"pose\", \"significant\"have the best score 0.15350567286626973\n",
        "7. In doc7, the words \"good\" has the best score 0.1485538769673578\n",
        "8. In doc8, the word \"management\" has the best score 0.22991970177630003\n",
        "9. In doc9, the words \"begin\", \"discuss\", \"weave\", \"federal\" have the best score 0.17712193023031123\n",
        "10. In doc10, the words \"ability\", \"actually\", \"affect\", \"base\", \"past\", \"similar\", \"program\", \"given\", \"deliver\", \"nothing\", \"actionable\", \"achieve\", \"devolution\" have the best score 0.1001123953475672\n",
        "\n",
        "From all documents, we can see that three words that have the best score of all documents are perspective (0.35424386046062245), issue (0.28782313662425574), and management (0.22991970177630003)\n",
        "\n",
        "Consideration:\n",
        "the words with the best TF*IDF score indicate the most important words across the whole corpus\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SoS_xfAMK1a",
        "colab_type": "text"
      },
      "source": [
        "The first distance metric that I used is Jaccard Distance or also commonly known as intersection over union. This metric defines as size of intersection divided by size of union of two sets. The greater the proportion of intersections, the more similar the two documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bwnke_w2AcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "2f323c04-30ef-483c-d832-fa97c54f789d"
      },
      "source": [
        "#Using Jaccard Index\n",
        "def jaccard_similarity(doc1,doc2):\n",
        "  # calculate the intersection\n",
        "  num_intersection = len(set.intersection(doc1, doc2))\n",
        "  # calculate the union\n",
        "  num_union = len(set.union(doc1, doc2))\n",
        "  # calculate the Jaccard index, return 1 if the set is empty\n",
        "  try:\n",
        "      return float(num_intersection) / num_union\n",
        "  except ZeroDivisionError:\n",
        "      return 1.0\n",
        "\n",
        "print(\"Jaccard(doc1, doc2) = \", jaccard_similarity(doc1, doc2))\n",
        "print(\"Jaccard(doc1, doc3) = \", jaccard_similarity(doc1, doc3))\n",
        "print(\"Jaccard(doc1, doc4) = \", jaccard_similarity(doc1, doc4))\n",
        "print(\"Jaccard(doc1, doc5) = \", jaccard_similarity(doc1, doc5))\n",
        "print(\"Jaccard(doc1, doc6) = \", jaccard_similarity(doc1, doc6))\n",
        "print(\"Jaccard(doc1, doc7) = \", jaccard_similarity(doc1, doc7))\n",
        "print(\"Jaccard(doc1, doc8) = \", jaccard_similarity(doc1, doc8))\n",
        "print(\"Jaccard(doc1, doc9) = \", jaccard_similarity(doc1, doc9))\n",
        "print(\"Jaccard(doc1, doc10) = \", jaccard_similarity(doc1, doc10))\n",
        "\n",
        "print(\"Jaccard(doc2, doc3) = \", jaccard_similarity(doc2, doc3))\n",
        "print(\"Jaccard(doc2, doc4) = \", jaccard_similarity(doc2, doc4))\n",
        "print(\"Jaccard(doc2, doc5) = \", jaccard_similarity(doc2, doc5))\n",
        "print(\"Jaccard(doc2, doc6) = \", jaccard_similarity(doc2, doc6))\n",
        "print(\"Jaccard(doc2, doc7) = \", jaccard_similarity(doc2, doc7))\n",
        "print(\"Jaccard(doc2, doc8) = \", jaccard_similarity(doc2, doc8))\n",
        "print(\"Jaccard(doc2, doc9) = \", jaccard_similarity(doc2, doc9))\n",
        "print(\"Jaccard(doc2, doc10) = \", jaccard_similarity(doc2, doc10))\n",
        "\n",
        "print(\"Jaccard(doc3, doc4) = \", jaccard_similarity(doc3, doc4))\n",
        "print(\"Jaccard(doc3, doc5) = \", jaccard_similarity(doc3, doc5))\n",
        "print(\"Jaccard(doc3, doc6) = \", jaccard_similarity(doc3, doc6))\n",
        "print(\"Jaccard(doc3, doc7) = \", jaccard_similarity(doc3, doc7))\n",
        "print(\"Jaccard(doc3, doc8) = \", jaccard_similarity(doc3, doc8))\n",
        "print(\"Jaccard(doc3, doc9) = \", jaccard_similarity(doc3, doc9))\n",
        "print(\"Jaccard(doc3, doc10) = \", jaccard_similarity(doc3, doc10))\n",
        "\n",
        "print(\"Jaccard(doc4, doc5) = \", jaccard_similarity(doc4, doc5))\n",
        "print(\"Jaccard(doc4, doc6) = \", jaccard_similarity(doc4, doc6))\n",
        "print(\"Jaccard(doc4, doc7) = \", jaccard_similarity(doc4, doc7))\n",
        "print(\"Jaccard(doc4, doc8) = \", jaccard_similarity(doc4, doc8))\n",
        "print(\"Jaccard(doc4, doc9) = \", jaccard_similarity(doc4, doc9))\n",
        "print(\"Jaccard(doc4, doc10) = \", jaccard_similarity(doc4, doc10))\n",
        "\n",
        "print(\"Jaccard(doc5, doc6) = \", jaccard_similarity(doc5, doc6))\n",
        "print(\"Jaccard(doc5, doc7) = \", jaccard_similarity(doc5, doc7))\n",
        "print(\"Jaccard(doc5, doc8) = \", jaccard_similarity(doc5, doc8))\n",
        "print(\"Jaccard(doc5, doc9) = \", jaccard_similarity(doc5, doc9))\n",
        "print(\"Jaccard(doc5, doc10) = \", jaccard_similarity(doc5, doc10))\n",
        "\n",
        "print(\"Jaccard(doc6, doc7) = \", jaccard_similarity(doc6, doc7))\n",
        "print(\"Jaccard(doc6, doc8) = \", jaccard_similarity(doc6, doc8))\n",
        "print(\"Jaccard(doc6, doc9) = \", jaccard_similarity(doc6, doc9))\n",
        "print(\"Jaccard(doc6, doc10) = \", jaccard_similarity(doc6, doc10))\n",
        "\n",
        "print(\"Jaccard(doc7, doc8) = \", jaccard_similarity(doc7, doc8))\n",
        "print(\"Jaccard(doc7, doc9) = \", jaccard_similarity(doc7, doc9))\n",
        "print(\"Jaccard(doc7, doc10) = \", jaccard_similarity(doc7, doc10))\n",
        "\n",
        "print(\"Jaccard(doc8, doc9) = \", jaccard_similarity(doc8, doc9))\n",
        "print(\"Jaccard(doc8, doc10) = \", jaccard_similarity(doc8, doc10))\n",
        "\n",
        "print(\"Jaccard(doc9, doc10) = \", jaccard_similarity(doc9, doc10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jaccard(doc1, doc2) =  0.09523809523809523\n",
            "Jaccard(doc1, doc3) =  0.18181818181818182\n",
            "Jaccard(doc1, doc4) =  0.0\n",
            "Jaccard(doc1, doc5) =  0.058823529411764705\n",
            "Jaccard(doc1, doc6) =  0.08333333333333333\n",
            "Jaccard(doc1, doc7) =  0.05263157894736842\n",
            "Jaccard(doc1, doc8) =  0.09090909090909091\n",
            "Jaccard(doc1, doc9) =  0.043478260869565216\n",
            "Jaccard(doc1, doc10) =  0.0625\n",
            "Jaccard(doc2, doc3) =  0.08\n",
            "Jaccard(doc2, doc4) =  0.0\n",
            "Jaccard(doc2, doc5) =  0.027777777777777776\n",
            "Jaccard(doc2, doc6) =  0.08\n",
            "Jaccard(doc2, doc7) =  0.0\n",
            "Jaccard(doc2, doc8) =  0.041666666666666664\n",
            "Jaccard(doc2, doc9) =  0.08695652173913043\n",
            "Jaccard(doc2, doc10) =  0.0\n",
            "Jaccard(doc3, doc4) =  0.0\n",
            "Jaccard(doc3, doc5) =  0.05263157894736842\n",
            "Jaccard(doc3, doc6) =  0.07142857142857142\n",
            "Jaccard(doc3, doc7) =  0.07317073170731707\n",
            "Jaccard(doc3, doc8) =  0.07692307692307693\n",
            "Jaccard(doc3, doc9) =  0.07692307692307693\n",
            "Jaccard(doc3, doc10) =  0.08571428571428572\n",
            "Jaccard(doc4, doc5) =  0.05714285714285714\n",
            "Jaccard(doc4, doc6) =  0.038461538461538464\n",
            "Jaccard(doc4, doc7) =  0.0\n",
            "Jaccard(doc4, doc8) =  0.0\n",
            "Jaccard(doc4, doc9) =  0.041666666666666664\n",
            "Jaccard(doc4, doc10) =  0.0\n",
            "Jaccard(doc5, doc6) =  0.1111111111111111\n",
            "Jaccard(doc5, doc7) =  0.038461538461538464\n",
            "Jaccard(doc5, doc8) =  0.0\n",
            "Jaccard(doc5, doc9) =  0.08571428571428572\n",
            "Jaccard(doc5, doc10) =  0.02127659574468085\n",
            "Jaccard(doc6, doc7) =  0.1\n",
            "Jaccard(doc6, doc8) =  0.037037037037037035\n",
            "Jaccard(doc6, doc9) =  0.07692307692307693\n",
            "Jaccard(doc6, doc10) =  0.08571428571428572\n",
            "Jaccard(doc7, doc8) =  0.07692307692307693\n",
            "Jaccard(doc7, doc9) =  0.0\n",
            "Jaccard(doc7, doc10) =  0.08333333333333333\n",
            "Jaccard(doc8, doc9) =  0.0\n",
            "Jaccard(doc8, doc10) =  0.09090909090909091\n",
            "Jaccard(doc9, doc10) =  0.125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx0t9mePDsoH",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can conclude that:\n",
        "1. doc1 has the most similar to doc3\n",
        "2. doc2 has the most similar to doc9\n",
        "3. doc3 has the most similar to doc1\n",
        "4. doc4 has the most similar to doc5\n",
        "5. doc5 has the most similar to doc6\n",
        "6. doc6 has the most similar to doc5\n",
        "7. doc7 has the most similar to doc10\n",
        "8. doc8 has the most similar to doc10\n",
        "9. doc9 has the most similar to doc10\n",
        "10. doc10 has the most similar to doc9\n",
        "\n",
        "The similar documents are doc1 and doc3, because they have the best score of similarity based on Jaccard Similarity 0.18181818181818182\n",
        "\n",
        "Consideration: if the length of the documents is large and even we get some same words then jaccard distance is not doing well.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZQV3PEvN3FE",
        "colab_type": "text"
      },
      "source": [
        "The second distance metric that I used is Sørensen–Dice coefficient. This coefficient is not very different in form from the Jaccard index. Both are equivalent in the sense that given a value for the Sørensen–Dice coefficient S, one can calculate the respective Jaccard index value J and vice versa, using the equations J=S/(2-S) and S=2J/(1+J). \n",
        "\n",
        "The function ranges between zero and one, like Jaccard. In order to resolve the drawback of Jaccard Distance about the large document, Sørensen–Dice coefficient sets the denominator as the average value of the two sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBVfEYDD15NP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "5a1f5c7e-4150-485b-a011-682ce57b8536"
      },
      "source": [
        "#using Sørensen–Dice coefficient\n",
        "def dice_similarity(doc1, doc2):\n",
        "  num_intersection =  len(set.intersection(doc1, doc2))\n",
        "  sum_nums = len(doc1) + len(doc2)\n",
        "  try:\n",
        "    return 2 * num_intersection / sum_nums\n",
        "  except ZeroDivisionError:\n",
        "    return 1.0\n",
        "\n",
        "print(\"Dice(doc1, doc2) = \", dice_similarity(doc1, doc2))\n",
        "print(\"Dice(doc1, doc3) = \", dice_similarity(doc1, doc3))\n",
        "print(\"Dice(doc1, doc4) = \", dice_similarity(doc1, doc4))\n",
        "print(\"Dice(doc1, doc5) = \", dice_similarity(doc1, doc5))\n",
        "print(\"Dice(doc1, doc6) = \", dice_similarity(doc1, doc6))\n",
        "print(\"Dice(doc1, doc7) = \", dice_similarity(doc1, doc7))\n",
        "print(\"Dice(doc1, doc8) = \", dice_similarity(doc1, doc8))\n",
        "print(\"Dice(doc1, doc9) = \", dice_similarity(doc1, doc9))\n",
        "print(\"Dice(doc1, doc10) = \", dice_similarity(doc1, doc10))\n",
        "\n",
        "print(\"Dice(doc2, doc3) = \", dice_similarity(doc2, doc3))\n",
        "print(\"Dice(doc2, doc4) = \", dice_similarity(doc2, doc4))\n",
        "print(\"Dice(doc2, doc5) = \", dice_similarity(doc2, doc5))\n",
        "print(\"Dice(doc2, doc6) = \", dice_similarity(doc2, doc6))\n",
        "print(\"Dice(doc2, doc7) = \", dice_similarity(doc2, doc7))\n",
        "print(\"Dice(doc2, doc8) = \", dice_similarity(doc2, doc8))\n",
        "print(\"Dice(doc2, doc9) = \", dice_similarity(doc2, doc9))\n",
        "print(\"Dice(doc2, doc10) = \", dice_similarity(doc2, doc10))\n",
        "\n",
        "print(\"Dice(doc3, doc4) = \", dice_similarity(doc3, doc4))\n",
        "print(\"Dice(doc3, doc5) = \", dice_similarity(doc3, doc5))\n",
        "print(\"Dice(doc3, doc6) = \", dice_similarity(doc3, doc6))\n",
        "print(\"Dice(doc3, doc7) = \", dice_similarity(doc3, doc7))\n",
        "print(\"Dice(doc3, doc8) = \", dice_similarity(doc3, doc8))\n",
        "print(\"Dice(doc3, doc9) = \", dice_similarity(doc3, doc9))\n",
        "print(\"Dice(doc3, doc10) = \", dice_similarity(doc3, doc10))\n",
        "\n",
        "print(\"Dice(doc4, doc5) = \", dice_similarity(doc4, doc5))\n",
        "print(\"Dice(doc4, doc6) = \", dice_similarity(doc4, doc6))\n",
        "print(\"Dice(doc4, doc7) = \", dice_similarity(doc4, doc7))\n",
        "print(\"Dice(doc4, doc8) = \", dice_similarity(doc4, doc8))\n",
        "print(\"Dice(doc4, doc9) = \", dice_similarity(doc4, doc9))\n",
        "print(\"Dice(doc4, doc10) = \", dice_similarity(doc4, doc10))\n",
        "\n",
        "print(\"Dice(doc5, doc6) = \", dice_similarity(doc5, doc6))\n",
        "print(\"Dice(doc5, doc7) = \", dice_similarity(doc5, doc7))\n",
        "print(\"Dice(doc5, doc8) = \", dice_similarity(doc5, doc8))\n",
        "print(\"Dice(doc5, doc9) = \", dice_similarity(doc5, doc9))\n",
        "print(\"Dice(doc5, doc10) = \", dice_similarity(doc5, doc10))\n",
        "\n",
        "print(\"Dice(doc6, doc7) = \", dice_similarity(doc6, doc7))\n",
        "print(\"Dice(doc6, doc8) = \", dice_similarity(doc6, doc8))\n",
        "print(\"Dice(doc6, doc9) = \", dice_similarity(doc6, doc9))\n",
        "print(\"Dice(doc6, doc10) = \", dice_similarity(doc6, doc10))\n",
        "\n",
        "print(\"Dice(doc7, doc8) = \", dice_similarity(doc7, doc8))\n",
        "print(\"Dice(doc7, doc9) = \", dice_similarity(doc7, doc9))\n",
        "print(\"Dice(doc7, doc10) = \", dice_similarity(doc7, doc10))\n",
        "\n",
        "print(\"Dice(doc8, doc9) = \", dice_similarity(doc8, doc9))\n",
        "print(\"Dice(doc8, doc10) = \", dice_similarity(doc8, doc10))\n",
        "\n",
        "print(\"Dice(doc9, doc10) = \", dice_similarity(doc9, doc10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dice(doc1, doc2) =  0.17391304347826086\n",
            "Dice(doc1, doc3) =  0.3076923076923077\n",
            "Dice(doc1, doc4) =  0.0\n",
            "Dice(doc1, doc5) =  0.1111111111111111\n",
            "Dice(doc1, doc6) =  0.15384615384615385\n",
            "Dice(doc1, doc7) =  0.1\n",
            "Dice(doc1, doc8) =  0.16666666666666666\n",
            "Dice(doc1, doc9) =  0.08333333333333333\n",
            "Dice(doc1, doc10) =  0.11764705882352941\n",
            "Dice(doc2, doc3) =  0.14814814814814814\n",
            "Dice(doc2, doc4) =  0.0\n",
            "Dice(doc2, doc5) =  0.05405405405405406\n",
            "Dice(doc2, doc6) =  0.14814814814814814\n",
            "Dice(doc2, doc7) =  0.0\n",
            "Dice(doc2, doc8) =  0.08\n",
            "Dice(doc2, doc9) =  0.16\n",
            "Dice(doc2, doc10) =  0.0\n",
            "Dice(doc3, doc4) =  0.0\n",
            "Dice(doc3, doc5) =  0.1\n",
            "Dice(doc3, doc6) =  0.13333333333333333\n",
            "Dice(doc3, doc7) =  0.13636363636363635\n",
            "Dice(doc3, doc8) =  0.14285714285714285\n",
            "Dice(doc3, doc9) =  0.14285714285714285\n",
            "Dice(doc3, doc10) =  0.15789473684210525\n",
            "Dice(doc4, doc5) =  0.10810810810810811\n",
            "Dice(doc4, doc6) =  0.07407407407407407\n",
            "Dice(doc4, doc7) =  0.0\n",
            "Dice(doc4, doc8) =  0.0\n",
            "Dice(doc4, doc9) =  0.08\n",
            "Dice(doc4, doc10) =  0.0\n",
            "Dice(doc5, doc6) =  0.2\n",
            "Dice(doc5, doc7) =  0.07407407407407407\n",
            "Dice(doc5, doc8) =  0.0\n",
            "Dice(doc5, doc9) =  0.15789473684210525\n",
            "Dice(doc5, doc10) =  0.041666666666666664\n",
            "Dice(doc6, doc7) =  0.18181818181818182\n",
            "Dice(doc6, doc8) =  0.07142857142857142\n",
            "Dice(doc6, doc9) =  0.14285714285714285\n",
            "Dice(doc6, doc10) =  0.15789473684210525\n",
            "Dice(doc7, doc8) =  0.14285714285714285\n",
            "Dice(doc7, doc9) =  0.0\n",
            "Dice(doc7, doc10) =  0.15384615384615385\n",
            "Dice(doc8, doc9) =  0.0\n",
            "Dice(doc8, doc10) =  0.16666666666666666\n",
            "Dice(doc9, doc10) =  0.2222222222222222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Pn55UOGNUl",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can conclude that:\n",
        "1. doc1 has the most similar to doc3\n",
        "2. doc2 has the most similar to doc9\n",
        "3. doc3 has the most similar to doc1\n",
        "4. doc4 has the most similar to doc5\n",
        "5. doc5 has the most similar to doc6\n",
        "6. doc6 has the most similar to doc5\n",
        "7. doc7 has the most similar to doc10\n",
        "8. doc8 has the most similar to doc10\n",
        "9. doc9 has the most similar to doc10\n",
        "10. doc10 has the most similar to doc9\n",
        "\n",
        "The similar documents are doc1 and doc3, because they have the best score of similarity based on Sørensen–Dice Similarity 0.3076923076923077\n",
        "\n",
        "Consideration: \n",
        "The Sørensen–Dice coefficient is useful for ecological community data [1]. Justification for its use is primarily empirical rather than theoretical (although it can be justified theoretically as the intersection of two fuzzy sets[2]). As compared to Euclidean distance, the Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.[3] Recently the Dice score (and its variations, e.g. logDice taking a logarithm of it) has become popular in computer lexicography for measuring the lexical association score of two given words.[4] It is also commonly used in image segmentation, in particular for comparing algorithm output against reference masks in medical applications.[5]\n",
        "\n",
        "References:\n",
        "1. Looman, J.; Campbell, J.B. (1960). \"Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation\". Ecology. 41 (3): 409–416. doi:10.2307/1933315. JSTOR 1933315.\n",
        "2. Roberts, D.W. (1986). \"Ordination on the basis of fuzzy set theory\". Vegetatio. 66 (3): 123–131. doi:10.1007/BF00039905\n",
        "3.  McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6\n",
        "4.  Rychlý, P. (2008) A lexicographer-friendly association score. Proceedings of the Second Workshop on Recent Advances in Slavonic Natural Language Processing RASLAN 2008: 6–9\n",
        "5. Zijdenbos, A.P.; Dawant, B.M.; Margolin, R.A.; Palmer, A.C. (1994). \"Morphometric analysis of white matter lesions in MR images: method and validation\". IEEE Transactions on Medical Imaging. Institute of Electrical and Electronics Engineers (IEEE). 13 (4): 716–724. doi:10.1109/42.363096\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U7b0a9NRQVS",
        "colab_type": "text"
      },
      "source": [
        "The third distance metric that I used is Szymkiewicz-Simpson coefficient or popular as overlap coefficient. This method is defined as the size of the union of set A and set B over the size of the smaller set between A and B. \n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AGt4zw8BwqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "39d1893b-7a62-4015-e0f7-314b9ebc96ca"
      },
      "source": [
        "#Using Szymkiewicz-Simpson coefficient\n",
        "\n",
        "def simpson_similarity(list_a, list_b):\n",
        "  num_intersection = len(set.intersection(set(list_a), set(list_b)))\n",
        "  min_num = min(len(set(list_a)), len(set(list_b)))\n",
        "  try:\n",
        "    return num_intersection / min_num\n",
        "  except ZeroDivisionError:\n",
        "    if num_intersection == 0:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "print(\"Simpson(doc1, doc2) = \", simpson_similarity(doc1, doc2))\n",
        "print(\"Simpson(doc1, doc3) = \", simpson_similarity(doc1, doc3))\n",
        "print(\"Simpson(doc1, doc4) = \", simpson_similarity(doc1, doc4))\n",
        "print(\"Simpson(doc1, doc5) = \", simpson_similarity(doc1, doc5))\n",
        "print(\"Simpson(doc1, doc6) = \", simpson_similarity(doc1, doc6))\n",
        "print(\"Simpson(doc1, doc7) = \", simpson_similarity(doc1, doc7))\n",
        "print(\"Simpson(doc1, doc8) = \", simpson_similarity(doc1, doc8))\n",
        "print(\"Simpson(doc1, doc9) = \", simpson_similarity(doc1, doc9))\n",
        "print(\"Simpson(doc1, doc10) = \", simpson_similarity(doc1, doc10))\n",
        "\n",
        "print(\"Simpson(doc2, doc3) = \", simpson_similarity(doc2, doc3))\n",
        "print(\"Simpson(doc2, doc4) = \", simpson_similarity(doc2, doc4))\n",
        "print(\"Simpson(doc2, doc5) = \", simpson_similarity(doc2, doc5))\n",
        "print(\"Simpson(doc2, doc6) = \", simpson_similarity(doc2, doc6))\n",
        "print(\"Simpson(doc2, doc7) = \", simpson_similarity(doc2, doc7))\n",
        "print(\"Simpson(doc2, doc8) = \", simpson_similarity(doc2, doc8))\n",
        "print(\"Simpson(doc2, doc9) = \", simpson_similarity(doc2, doc9))\n",
        "print(\"Simpson(doc2, doc10) = \", simpson_similarity(doc2, doc10))\n",
        "\n",
        "print(\"Simpson(doc3, doc4) = \", simpson_similarity(doc3, doc4))\n",
        "print(\"Simpson(doc3, doc5) = \", simpson_similarity(doc3, doc5))\n",
        "print(\"Simpson(doc3, doc6) = \", simpson_similarity(doc3, doc6))\n",
        "print(\"Simpson(doc3, doc7) = \", simpson_similarity(doc3, doc7))\n",
        "print(\"Simpson(doc3, doc8) = \", simpson_similarity(doc3, doc8))\n",
        "print(\"Simpson(doc3, doc9) = \", simpson_similarity(doc3, doc9))\n",
        "print(\"Simpson(doc3, doc10) = \", simpson_similarity(doc3, doc10))\n",
        "\n",
        "print(\"Simpson(doc4, doc5) = \", simpson_similarity(doc4, doc5))\n",
        "print(\"Simpson(doc4, doc6) = \", simpson_similarity(doc4, doc6))\n",
        "print(\"Simpson(doc4, doc7) = \", simpson_similarity(doc4, doc7))\n",
        "print(\"Simpson(doc4, doc8) = \", simpson_similarity(doc4, doc8))\n",
        "print(\"Simpson(doc4, doc9) = \", simpson_similarity(doc4, doc9))\n",
        "print(\"Simpson(doc4, doc10) = \", simpson_similarity(doc4, doc10))\n",
        "\n",
        "print(\"Simpson(doc5, doc6) = \", simpson_similarity(doc5, doc6))\n",
        "print(\"Simpson(doc5, doc7) = \", simpson_similarity(doc5, doc7))\n",
        "print(\"Simpson(doc5, doc8) = \", simpson_similarity(doc5, doc8))\n",
        "print(\"Simpson(doc5, doc9) = \", simpson_similarity(doc5, doc9))\n",
        "print(\"Simpson(doc5, doc10) = \", simpson_similarity(doc5, doc10))\n",
        "\n",
        "print(\"Simpson(doc6, doc7) = \", simpson_similarity(doc6, doc7))\n",
        "print(\"Simpson(doc6, doc8) = \", simpson_similarity(doc6, doc8))\n",
        "print(\"Simpson(doc6, doc9) = \", simpson_similarity(doc6, doc9))\n",
        "print(\"Simpson(doc6, doc10) = \", simpson_similarity(doc6, doc10))\n",
        "\n",
        "print(\"Simpson(doc7, doc8) = \", simpson_similarity(doc7, doc8))\n",
        "print(\"Simpson(doc7, doc9) = \", simpson_similarity(doc7, doc9))\n",
        "print(\"Simpson(doc7, doc10) = \", simpson_similarity(doc7, doc10))\n",
        "\n",
        "print(\"Simpson(doc8, doc9) = \", simpson_similarity(doc8, doc9))\n",
        "print(\"Simpson(doc8, doc10) = \", simpson_similarity(doc8, doc10))\n",
        "\n",
        "print(\"Simpson(doc9, doc10) = \", simpson_similarity(doc9, doc10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simpson(doc1, doc2) =  0.18181818181818182\n",
            "Simpson(doc1, doc3) =  0.36363636363636365\n",
            "Simpson(doc1, doc4) =  0.0\n",
            "Simpson(doc1, doc5) =  0.18181818181818182\n",
            "Simpson(doc1, doc6) =  0.18181818181818182\n",
            "Simpson(doc1, doc7) =  0.18181818181818182\n",
            "Simpson(doc1, doc8) =  0.18181818181818182\n",
            "Simpson(doc1, doc9) =  0.09090909090909091\n",
            "Simpson(doc1, doc10) =  0.18181818181818182\n",
            "Simpson(doc2, doc3) =  0.16666666666666666\n",
            "Simpson(doc2, doc4) =  0.0\n",
            "Simpson(doc2, doc5) =  0.08333333333333333\n",
            "Simpson(doc2, doc6) =  0.16666666666666666\n",
            "Simpson(doc2, doc7) =  0.0\n",
            "Simpson(doc2, doc8) =  0.08333333333333333\n",
            "Simpson(doc2, doc9) =  0.16666666666666666\n",
            "Simpson(doc2, doc10) =  0.0\n",
            "Simpson(doc3, doc4) =  0.0\n",
            "Simpson(doc3, doc5) =  0.13333333333333333\n",
            "Simpson(doc3, doc6) =  0.13333333333333333\n",
            "Simpson(doc3, doc7) =  0.2\n",
            "Simpson(doc3, doc8) =  0.15384615384615385\n",
            "Simpson(doc3, doc9) =  0.15384615384615385\n",
            "Simpson(doc3, doc10) =  0.2\n",
            "Simpson(doc4, doc5) =  0.16666666666666666\n",
            "Simpson(doc4, doc6) =  0.08333333333333333\n",
            "Simpson(doc4, doc7) =  0.0\n",
            "Simpson(doc4, doc8) =  0.0\n",
            "Simpson(doc4, doc9) =  0.08333333333333333\n",
            "Simpson(doc4, doc10) =  0.0\n",
            "Simpson(doc5, doc6) =  0.26666666666666666\n",
            "Simpson(doc5, doc7) =  0.08\n",
            "Simpson(doc5, doc8) =  0.0\n",
            "Simpson(doc5, doc9) =  0.23076923076923078\n",
            "Simpson(doc5, doc10) =  0.043478260869565216\n",
            "Simpson(doc6, doc7) =  0.26666666666666666\n",
            "Simpson(doc6, doc8) =  0.07692307692307693\n",
            "Simpson(doc6, doc9) =  0.15384615384615385\n",
            "Simpson(doc6, doc10) =  0.2\n",
            "Simpson(doc7, doc8) =  0.23076923076923078\n",
            "Simpson(doc7, doc9) =  0.0\n",
            "Simpson(doc7, doc10) =  0.17391304347826086\n",
            "Simpson(doc8, doc9) =  0.0\n",
            "Simpson(doc8, doc10) =  0.23076923076923078\n",
            "Simpson(doc9, doc10) =  0.3076923076923077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j2rkco5LLfU",
        "colab_type": "text"
      },
      "source": [
        "From the result above, we can conclude that:\n",
        "1. doc1 has the most similar to doc3\n",
        "2. doc2 has the most similar to doc3, doc6, and doc9\n",
        "3. doc3 has the most similar to doc1\n",
        "4. doc4 has the most similar to doc5\n",
        "5. doc5 has the most similar to doc6\n",
        "6. doc6 has the most similar to doc5 and doc7\n",
        "7. doc7 has the most similar to doc6\n",
        "8. doc8 has the most similar to doc10\n",
        "9. doc9 has the most similar to doc10\n",
        "10. doc10 has the most similar to doc9\n",
        "\n",
        "The similar documents are doc1 and doc3, because they have the best score of similarity based on Szymkiewicz-Simpson Similarity 0.36363636363636365\n",
        "\n",
        "Consideration: \n",
        "Overlap Coefficient can provide better insight into how similar two documents, how similar the document of neighbors are. By knowing the sizes of each document, an analyst can easily know if one document is a proper subset (full contained) in the other document."
      ]
    }
  ]
}